{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "42ea5318",
      "metadata": {
        "id": "42ea5318"
      },
      "source": [
        "# **SCI 111 - Zomato Recommendation - Data Analysis**\n",
        "\n",
        "## **Authors of this Repository**\n",
        "- **Binwag, Louis G. III** - [@louisuwie](https://github.com/louis-uwie)\n",
        "- **Lozada, Godwyn Idris L.** - [@godwynlozada](https://github.com/godwynlozada)\n",
        "\n",
        "<!-- CSCI 111 is a class that teaches how Artificial Intelligence (AI) think.\n",
        "The class first dwells into basic data structures that concern with how AI may approach decision making processes (i.e., Breadth-First-Search, Depth-First-Search, A* Algorithm, etc.).\n",
        "\n",
        "Data structures help us understand and visualize easier how \"problem solving Agents\" think. These are classifications of Agents that have a degree of what they can essentially 'solve' (i.e., Problem-Based Agent, Supervised or Unsupervised Agents, etc.) ...\n",
        "\n",
        "After understanding solving Agents, we move into the basics of Machine Learning which includes Data Transformation, Decision Trees, K-Nearest Neighbours (kNN), and Clustering. This is where we first try Jupyter in programming a script that uses various models in transforming, manipulating, and analysing datasets.\n",
        "\n",
        "Lastly, we tackle Logical Agents. This is adding more logic towards what we previously learned as problem solving Agents where we use propositional logic, inferences, and entailment to establish how we can visualize how an Agent might look at a problem (i.e., Truth table, Inference diagrams, Logical statements / sentences, Knowledge Base (KB)) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da74d1d4",
      "metadata": {
        "id": "da74d1d4"
      },
      "source": [
        "# **I. Initial Set Up**\n",
        "\n",
        "## **Project on Evaluating Machine Learning Models.**\n",
        "\n",
        "**Instructions.** Select a dataset from [UCI](https://archive.ics.uci.edu/ml/datasets.php) or [Google](https://datasetsearch.research.google.com/), formulate a machine learning problem (supervised or unsupervised), and build and evaluate two models (different methods) that solve the problem. Any programming language may be used.\n",
        "- You may also use other legitimate sources at the same level of the UCI and Google sites provided.\n",
        "- You may use methods not taught in class. KNN is not an option.\n",
        "- You may also use a portion of the dataset if its size causes problems (e.g. reduce the number of rows)\n",
        "\n",
        "**Deliverables.** In a Google Drive folder that I can access, submit the following:\n",
        "- Source code and executables\n",
        "- Instructions on how to use your resources (i.e. your program)\n",
        "- Slide deck explaining your work\n",
        "- Recorded video presentation of your work (approx 20-30mins)\n",
        "\n",
        "**Expected Output.**\n",
        "- Jupyter Notebook (.ipynb)\n",
        "- Resources (csv unclean and cleaned)\n",
        "- Video Presentation\n",
        "- Slide Deck Presentation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a15b589b",
      "metadata": {
        "id": "a15b589b"
      },
      "source": [
        "# **II. Data Set**\n",
        "\n",
        "**Dataset Overview.**\n",
        "\n",
        "The dataset contains raw information sourced from the Zomato Recommendation Platform for restaurants based in Pune, India, covering the year 2023. Each row corresponds to a single restaurant entry and includes a variety of attributes such as the restaurant’s name, multiple types of cuisine offered (up to eight slots), its categorized food type, the average cost for two people, the locality within Pune, and the average customer dining rating.\n",
        "\n",
        "This dataset provides a foundation for predictive modeling and exploratory analysis, as it blends both categorical (e.g., cuisine types, locality) and numerical (e.g., rating, pricing) data. Through this structure, we can investigate patterns in consumer preferences, identify key factors influencing restaurant ratings, and evaluate the performance of machine learning models like Decision Trees and Mixed Naive Bayes in classifying highly rated restaurants.\n",
        "\n",
        "| **Features**              | **Short Explanation**                                                         | **Possible Values / Example**                 |\n",
        "| ------------------------ | ----------------------------------------------------------------------------- | --------------------------------------------- |\n",
        "| `Restaurant_Name`        | Name of the restaurant listed on Zomato                                       | `\"Le Plaisir\"`, `\"Savya Rasa\"`                |\n",
        "| `Cuisine1` to `Cuisine8` | Different types of cuisines offered by the restaurant, in order of prominence | `\"South Indian\"`, `\"Desserts\"`, `\"MISSING\"`   |\n",
        "| `Category`               | Grouped categories combining all cuisine types into a readable list           | `\"Cafe, Italian, Continental...\"`             |\n",
        "| `Pricing_for_2`          | Approximate cost for two people, in INR                                       | `600`, `1200`, `2100`                         |\n",
        "| `Locality in Pune`       | Location/neighborhood of the restaurant in Pune                               | `\"Koregaon Park\"`, `\"Baner\"`, `\"Viman Nagar\"` |\n",
        "| `Dining_Rating`          | Average customer rating of the restaurant (out of 5)                          | `4.2`, `3.8`, `4.9`                           |\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4480d0a8-e8d5-4114-8561-d17f7a9d5faa",
      "metadata": {
        "id": "4480d0a8-e8d5-4114-8561-d17f7a9d5faa"
      },
      "source": [
        "# **III. Ideal Pipeline**\n",
        "\n",
        "Our goal for this analysis is to be able to determine which model is able to more accurately predict what are the top restaurants in the locale (possibly depending on cuisines, locality, or average price.) <!-- Expound >\n",
        "\n",
        "**1. Data Preprocessing**\n",
        "- Load and Inspection of data.\n",
        "- Cleaning the data (i.e. Tableau) <!-- care of Godwyn -->\n",
        "\n",
        "**2. Exploratory Data Analysis (EDA)**\n",
        "- This will be more on understanding which features create a reactive effect towards the rest of the feature.\n",
        "- Identifies which feature is able to change the course of the data. From there, we will implement the models.\n",
        "\n",
        "**3. Decision Tree Implementation 1 (DT1)**\n",
        "- This will be one of the initial basis of our model apart from EDA.\n",
        "\n",
        "**4. Apply Decision Tree Implementation 2 (DT2)**\n",
        "- The second implementation of Decision Tree will consist of the data set where we have omitted certain features (To be identified soon. _i.e., MISSING values, certain irrelevant features_) based on our domain knowledge.\n",
        "- Comparing this to Decision Tree Implementation 1, we may be able to justify that omitting certain \"junk\" features can make Decision Tree model more accurate.\n",
        "\n",
        "**5. Apply Mixed Naive Bayes (MNB)**\n",
        "- The final model we use in this study is the Mixed Naive Bayes (MNB) classifier. This model is a variation of the standard Naive Bayes algorithm that allows us to handle both categorical and continuous features—whic makes it especially well-suited for real-world datasets like Zomato’s, where variables such as cuisine type (categorical) and average price (numerical) coexist.\n",
        "\n",
        "**6. Conclusion**\n",
        "- Generally, through ***Exploratory Data Analysis (EDA) and both Decision Tree implementations***, you may conclude that certain features—such as Cuisine type, Locality, or Average Price—have a strong influence on whether a restaurant receives high ratings. _Features like 'MISSING' or non-informative columns could be confirmed as noise, negatively affecting model accuracy._\n",
        "- Comparing ***Decision Tree 1 (all features) with Decision Tree 2 (cleaned features)***, you might find that:\n",
        "    - Removing irrelevant or noisy features leads to higher accuracy and simpler tree structures.\n",
        "    - This supports the idea that domain knowledge-based feature pruning improves model performance.\n",
        "- ***Mixed Naive Bayes (MNB) might perform competitively or better on some metrics*** (like precision or recall) compared to Decision Trees, especially in cases where feature independence is mostly true. However, MNB might underperform if features are highly correlated, where Decision Trees can better handle interactions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b65b1a50",
      "metadata": {
        "id": "b65b1a50"
      },
      "source": [
        "# **IV. Data Preprocessing**\n",
        "\n",
        "< This section will include general importing and inspection of the data. Cleaning the data as well for nullified or duplicated values. > <!-- Expound more >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1f4b1f89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f4b1f89",
        "outputId": "71e83d41-0072-4d3b-cdd0-9cd1435a91c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting mixed-naive-bayes\n",
            "  Downloading mixed_naive_bayes-0.0.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading mixed_naive_bayes-0.0.3-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: mixed-naive-bayes\n",
            "Successfully installed mixed-naive-bayes-0.0.3\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n"
          ]
        }
      ],
      "source": [
        "## Assume that we do not have the necessary libraries installed.\n",
        "%pip install pandas numpy matplotlib seaborn scikit-learn mixed-naive-bayes graphviz #This is to install the libraries needed to run the code.\n",
        "%pip install --upgrade pip #Updates pip\n",
        "\n",
        "# Need to install tkinter.\n",
        "# For mac: brew install python-tk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import tkinter as tk\n",
        "# from tkinter import filedialog\n",
        "\n",
        "import math\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import percentileofscore\n",
        "from scipy.stats import f\n",
        "from scipy import stats\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold ## https://www.geeksforgeeks.org/cross-validation-machine-learning/\n",
        "from mixed_naive_bayes import MixedNB\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Use if you are using Google Drive\n",
        "import io\n",
        "\n",
        "## Use if you are using Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "7AwJY1bGEKb0",
        "outputId": "fcdc9a39-1159-463e-8137-cbd65a9e91d7"
      },
      "id": "7AwJY1bGEKb0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-851676ac-e107-4c8a-ac59-8cd53e50a6b6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-851676ac-e107-4c8a-ac59-8cd53e50a6b6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make this cell run before uploading the input file, please.\n",
        "\n",
        "If in case you uploaded the file before, please make sure to replace the file name in the file path.\n",
        "\n",
        "If you have not, kindly upload the data file."
      ],
      "metadata": {
        "id": "NdfOPO08Ms_I"
      },
      "id": "NdfOPO08Ms_I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ce5c023",
      "metadata": {
        "id": "3ce5c023"
      },
      "outputs": [],
      "source": [
        "## If using Jupyter Notebook / Run Locally via VS Code. Import the file LOZADA, BINWAG, CSCI 211 Zomato Dataset Pune.csv\n",
        "# Hardcoded file path to your dataset\n",
        "\n",
        "file_path = \"LOZADA, BINWAG, CSCI 211 Zomato Dataset Pune.csv\"\n",
        "zomato_pune = pd.read_csv(file_path)\n",
        "\n",
        "# Create a working copy for analysis\n",
        "zomato_for_eda = zomato_pune.copy()\n",
        "\n",
        "# Display the data\n",
        "zomato_for_eda.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16169d44",
      "metadata": {
        "id": "16169d44"
      },
      "outputs": [],
      "source": [
        "# Display the data\n",
        "zomato_for_eda.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90b2cb9",
      "metadata": {
        "id": "b90b2cb9"
      },
      "source": [
        "## **Restaurant Count per Locality**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b151c830",
      "metadata": {
        "id": "b151c830"
      },
      "outputs": [],
      "source": [
        "# Count restaurants per locality\n",
        "locality_counts = zomato_for_eda['Locality in Pune'].value_counts()\n",
        "\n",
        "# Plot the top 15\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=locality_counts.head(15).values, y=locality_counts.head(15).index, palette=\"viridis\")\n",
        "\n",
        "plt.title(\"Top 15 Localities by Number of Restaurants\")\n",
        "plt.xlabel(\"Number of Restaurants\")\n",
        "plt.ylabel(\"Locality\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1691fb74",
      "metadata": {
        "id": "1691fb74"
      },
      "source": [
        "## **Listing All Cuisines**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24cdb774",
      "metadata": {
        "id": "24cdb774"
      },
      "outputs": [],
      "source": [
        "# List of cuisine columns\n",
        "cuisine_cols = [f'Cuisine{i}' for i in range(1, 9)]\n",
        "\n",
        "# Flatten, drop NAs and \"MISSING\", then get unique values\n",
        "all_cuisines = pd.unique(\n",
        "    zomato_for_eda[cuisine_cols]\n",
        "    .values\n",
        "    .ravel()\n",
        ")\n",
        "\n",
        "# Clean list\n",
        "unique_cuisines = sorted([c for c in all_cuisines if pd.notna(c) and c != 'MISSING'])\n",
        "\n",
        "# Display\n",
        "print(\"Number of unique cuisines:\", len(unique_cuisines))\n",
        "print(unique_cuisines)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceec51c3",
      "metadata": {
        "id": "ceec51c3"
      },
      "source": [
        "## **Correlation between Pricing_for_2 and Dining_Rating.**\n",
        "This is to be able to understand if pricing is \"cheaper\" gains a better rating as a restaurant. However, this is just a shallow experiment as Pricing can't be the only factor in a high-rating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ede7d86-264e-4016-9fd4-6c8d7008ad14",
      "metadata": {
        "id": "7ede7d86-264e-4016-9fd4-6c8d7008ad14"
      },
      "outputs": [],
      "source": [
        "plt.hexbin(zomato_for_eda['Pricing_for_2'], zomato_for_eda['Dining_Rating'], gridsize=30, cmap='Blues')\n",
        "plt.colorbar(label='Count in Bin')\n",
        "plt.xlabel('Pricing for 2')\n",
        "plt.ylabel('Dining Rating')\n",
        "plt.title('Hexbin: Price vs Rating')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e023ca1",
      "metadata": {
        "id": "8e023ca1"
      },
      "outputs": [],
      "source": [
        "correlation = zomato_for_eda[['Pricing_for_2', 'Dining_Rating']].corr()\n",
        "print(\"Correlation between Pricing and Rating:\")\n",
        "print(correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8cea287",
      "metadata": {
        "id": "f8cea287"
      },
      "source": [
        "## **Correlation between Locality and Cuisine (1-8) to Dining_Rating.**\n",
        "We proceed to test if there is a correlation between a cuisine served in certain locality. Such that, if for instance, `Mediterranean` and `European` cuisines served in\t`Koregaon Park` receives a high rating whilst `Coffee` and `Desserts` served in the same locale has low-ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234ac8a2",
      "metadata": {
        "id": "234ac8a2"
      },
      "outputs": [],
      "source": [
        "# Reshape cuisine columns into one\n",
        "cuisine_cols = [f'Cuisine{i}' for i in range(1, 9)]\n",
        "\n",
        "# Melt cuisine columns\n",
        "long_df = zomato_for_eda.melt(\n",
        "    id_vars=['Dining_Rating', 'Locality in Pune'],\n",
        "    value_vars=cuisine_cols,\n",
        "    var_name='CuisineCol',\n",
        "    value_name='Cuisine'\n",
        ")\n",
        "\n",
        "# Drop missing cuisines\n",
        "long_df = long_df.dropna(subset=['Cuisine'])\n",
        "\n",
        "# Grouping\n",
        "rating_by_combo = (\n",
        "    long_df\n",
        "    .groupby(['Cuisine', 'Locality in Pune'])['Dining_Rating']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .rename(columns={'Dining_Rating': 'Avg_Rating'})\n",
        ")\n",
        "\n",
        "# Top 15 highest-rated cuisine-location combos.\n",
        "# Maintains the original index of the row.\n",
        "rating_by_combo.sort_values('Avg_Rating', ascending=False).head(15)\n",
        "\n",
        "# Revises the index starting from 0.\n",
        "# Uncomment if prefer to use original indexing.\n",
        "    # cleaned_top = rating_by_combo.sort_values('Avg_Rating', ascending=False).head(15).reset_index(drop=True)\n",
        "    # cleaned_top # Prints\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a097bcd3",
      "metadata": {
        "id": "a097bcd3"
      },
      "outputs": [],
      "source": [
        "# Filter to threshold. Only Avg_Rating over 3.8\n",
        "filtered = rating_by_combo[rating_by_combo['Avg_Rating'] > 3.8]\n",
        "\n",
        "# Keep only top cuisines and localities by frequency\n",
        "top_cuisines = (\n",
        "    long_df['Cuisine'].value_counts()\n",
        "    .loc[lambda x: x.index != 'MISSING']\n",
        "    .head(10).index\n",
        ")\n",
        "\n",
        "top_localities = long_df['Locality in Pune'].value_counts().head(10).index\n",
        "\n",
        "# Apply filter\n",
        "filtered = filtered[\n",
        "    (filtered['Cuisine'].isin(top_cuisines)) &\n",
        "    (filtered['Locality in Pune'].isin(top_localities))\n",
        "]\n",
        "\n",
        "# Pivot\n",
        "heatmap_data = filtered.pivot(\n",
        "    index=\"Cuisine\",\n",
        "    columns=\"Locality in Pune\",\n",
        "    values=\"Avg_Rating\"\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", linewidths=0.5, linecolor='gray')\n",
        "plt.title(\"Dining Rating > 3.8 — Top 10 Cuisines x Top 5 Localities\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c525cc96",
      "metadata": {
        "id": "c525cc96"
      },
      "outputs": [],
      "source": [
        "# Define the features and filter the DataFrame\n",
        "input_features = [\"Cuisine1\", \"Cuisine2\", \"Cuisine3\", \"Cuisine4\",\n",
        "                  \"Cuisine5\", \"Cuisine6\", \"Cuisine7\", \"Cuisine8\",\n",
        "                  \"Pricing_for_2\", \"Locality in Pune\"]\n",
        "\n",
        "X = zomato_for_eda.filter(items=input_features)\n",
        "\n",
        "# all cuisines\n",
        "cuisine_cols = [f'Cuisine{i}' for i in range(1, 9)]\n",
        "cuisines_flat = pd.Series(X[cuisine_cols].values.ravel())\n",
        "\n",
        "# Clean values\n",
        "cuisines_flat = cuisines_flat.replace(\"MISSING\", np.nan).dropna()\n",
        "\n",
        "# Count top 15\n",
        "cuisines_freq = cuisines_flat.value_counts().head(15)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=cuisines_freq.values, y=cuisines_freq.index)\n",
        "plt.title(\"Top 15 Most Common Cuisines\")\n",
        "plt.xlabel(\"Number of Occurrences\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7251b4f",
      "metadata": {
        "id": "e7251b4f"
      },
      "source": [
        "## **More EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76938ed",
      "metadata": {
        "id": "d76938ed"
      },
      "outputs": [],
      "source": [
        "# Set rating threshold\n",
        "rating_threshold = 3.75 ## Bhatia et. al. 2023\n",
        "\n",
        "# First-level helper functions\n",
        "def find_percent(df, feature_group, feature):\n",
        "    feature_filter = df.loc[df[feature_group] == feature]\n",
        "    if not feature_filter.empty:\n",
        "        percent_above_cutoff = 100 - percentileofscore(\n",
        "            feature_filter['Dining_Rating'], rating_threshold, kind='strict'\n",
        "        )\n",
        "    else:\n",
        "        percent_above_cutoff = 0\n",
        "    return percent_above_cutoff\n",
        "\n",
        "def find_mean_rating(df, feature_group, feature):\n",
        "    feature_filter = df.loc[df[feature_group] == feature]\n",
        "    return feature_filter['Dining_Rating'].mean() if not feature_filter.empty else np.nan\n",
        "\n",
        "# Second-level EDA helper\n",
        "def eda_resto_data_numerical(df, feature_group):\n",
        "    feature_list = pd.unique(df[feature_group]).tolist()\n",
        "    feature_list2 = [feature for feature in feature_list if feature != 'MISSING' and pd.notnull(feature)]\n",
        "\n",
        "    percent_exceeding_cutoff_feature = []\n",
        "    average_per_cutoff_feature = []\n",
        "\n",
        "    for feature in feature_list2:\n",
        "        above_threshold = round(find_percent(df, feature_group, feature), 4)\n",
        "        percent_exceeding_cutoff_feature.append(above_threshold)\n",
        "\n",
        "        mean_rating = round(find_mean_rating(df, feature_group, feature), 4)\n",
        "        average_per_cutoff_feature.append(mean_rating)\n",
        "\n",
        "    # MinMax scaling\n",
        "    scaler = MinMaxScaler()\n",
        "    mean_scaled = scaler.fit_transform(np.array(average_per_cutoff_feature).reshape(-1, 1)).flatten()\n",
        "    mean_scaled = [round(score, 4) for score in mean_scaled]\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    df_mean_feature = pd.DataFrame({\n",
        "        feature_group: feature_list2,\n",
        "        '% Above 3.75': percent_exceeding_cutoff_feature,\n",
        "        'Mean Rating': average_per_cutoff_feature,\n",
        "        'MinMax Scale Score': mean_scaled\n",
        "    })\n",
        "\n",
        "    df_mean_feature = df_mean_feature.sort_values(by=['Mean Rating', feature_group], ascending=[False, True]).reset_index(drop=True)\n",
        "    df_mean_feature['Rank'] = df_mean_feature.index + 1\n",
        "\n",
        "    return df_mean_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675a2fdc",
      "metadata": {
        "id": "675a2fdc"
      },
      "outputs": [],
      "source": [
        "eda_locality = eda_resto_data_numerical(zomato_for_eda, 'Locality in Pune')\n",
        "eda_locality.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c070a8",
      "metadata": {
        "id": "b9c070a8"
      },
      "outputs": [],
      "source": [
        "eda_resto_cuisine1 = eda_resto_data_numerical(zomato_for_eda, 'Cuisine1')\n",
        "eda_resto_cuisine1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d169160",
      "metadata": {
        "id": "5d169160"
      },
      "outputs": [],
      "source": [
        "eda_resto_cuisine2 = eda_resto_data_numerical(zomato_for_eda, 'Cuisine2')\n",
        "eda_resto_cuisine2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f169f34c",
      "metadata": {
        "id": "f169f34c"
      },
      "outputs": [],
      "source": [
        "eda_resto_cuisine3 = eda_resto_data_numerical(zomato_for_eda, 'Cuisine3')\n",
        "eda_resto_cuisine3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21fb2bec",
      "metadata": {
        "id": "21fb2bec"
      },
      "outputs": [],
      "source": [
        "eda_resto_cuisine4 = eda_resto_data_numerical(zomato_for_eda, 'Cuisine4')\n",
        "eda_resto_cuisine4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66cbdb9f",
      "metadata": {
        "id": "66cbdb9f"
      },
      "outputs": [],
      "source": [
        "eda_resto_cuisine5 = eda_resto_data_numerical(zomato_for_eda, 'Cuisine5')\n",
        "eda_resto_cuisine5.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3920f75b",
      "metadata": {
        "id": "3920f75b"
      },
      "outputs": [],
      "source": [
        "eda_resto_cuisine6 = eda_resto_data_numerical(zomato_for_eda, 'Cuisine6')\n",
        "eda_resto_cuisine6.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec4330d",
      "metadata": {
        "id": "eec4330d"
      },
      "outputs": [],
      "source": [
        "eda_resto_cuisine7 = eda_resto_data_numerical(zomato_for_eda, 'Cuisine7')\n",
        "eda_resto_cuisine7.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d06bd6",
      "metadata": {
        "id": "c7d06bd6"
      },
      "outputs": [],
      "source": [
        "eda_resto_cuisine8 = eda_resto_data_numerical(zomato_for_eda, 'Cuisine8')\n",
        "eda_resto_cuisine8.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377a92ed",
      "metadata": {
        "id": "377a92ed"
      },
      "outputs": [],
      "source": [
        "zomato_pune_yes = zomato_for_eda[zomato_for_eda['Dining_Rating'] >= 3.8]\n",
        "zomato_pune_yes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf530dc",
      "metadata": {
        "id": "9bf530dc"
      },
      "outputs": [],
      "source": [
        "zomato_pune_no = zomato_for_eda[zomato_for_eda['Dining_Rating'] < 3.8]\n",
        "zomato_pune_no.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db27aec",
      "metadata": {
        "id": "5db27aec"
      },
      "outputs": [],
      "source": [
        "min_cost = min(zomato_pune_yes['Pricing_for_2'])\n",
        "max_cost = max(zomato_pune_yes['Pricing_for_2'])\n",
        "\n",
        "num_bins = int((max_cost - min_cost) / 50) + 1\n",
        "num_bins\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.histplot(zomato_pune_yes['Pricing_for_2'], bins = num_bins, color='magenta')\n",
        "plt.title(\"Meal Cost Distribution of the Zomato Dataset in Pune (if rating >= 3.8)\")\n",
        "plt.xlabel(\"Cost of Meal for 2 (INR)\")\n",
        "plt.xlim(0, 5000)\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd55d13",
      "metadata": {
        "id": "9fd55d13"
      },
      "outputs": [],
      "source": [
        "min_cost = min(zomato_pune_no['Pricing_for_2'])\n",
        "max_cost = max(zomato_pune_no['Pricing_for_2'])\n",
        "\n",
        "num_bins = int((max_cost - min_cost) / 50) + 1\n",
        "num_bins\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.histplot(zomato_pune_no['Pricing_for_2'], bins = num_bins, color='red')\n",
        "plt.title(\"Meal Cost Distribution of the Zomato Dataset in Pune (if rating < 3.8)\")\n",
        "plt.xlabel(\"Cost of Meal for 2 (INR)\")\n",
        "plt.xlim(0, 5000)\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Descriptive and Inferential statistics**\n",
        "For the two subgroups of the dataset, we take the mean price (in Indian Rupees), standard deviation and number of instances per classification in this dataset. Then we do the unequal variances t-test."
      ],
      "metadata": {
        "id": "RtCsuyWrOgBH"
      },
      "id": "RtCsuyWrOgBH"
    },
    {
      "cell_type": "code",
      "source": [
        "zomato_price_overall = zomato_for_eda['Pricing_for_2']\n",
        "zomato_prices_yes = np.array(zomato_pune_yes['Pricing_for_2'])\n",
        "zomato_prices_no = np.array(zomato_pune_no['Pricing_for_2'])\n",
        "\n",
        "mean_prices_overall = np.mean(zomato_price_overall)\n",
        "mean_prices_yes = np.mean(zomato_prices_yes)\n",
        "mean_prices_no = np.mean(zomato_prices_no)\n",
        "\n",
        "stdev_prices_overall = np.std(zomato_price_overall, ddof=1) ## Sample standard deviation hence N - 1 degrees of freedom\n",
        "stdev_prices_yes = np.std(zomato_prices_yes, ddof=1) ## Sample standard deviation hence N - 1 degrees of freedom\n",
        "stdev_prices_no = np.std(zomato_prices_no, ddof=1)\n",
        "\n",
        "count_prices_overall = len(zomato_price_overall)\n",
        "count_prices_yes = len(zomato_prices_yes)\n",
        "count_prices_no = len(zomato_prices_no)\n",
        "\n",
        "\n",
        "print(\"Overall distribution\")\n",
        "print(f\"Mean Price: {mean_prices_overall:.2f}\")\n",
        "print(f\"Standard Deviation: {stdev_prices_overall:.2f}\")\n",
        "print(f\"Number of Instances: {count_prices_overall}\")\n",
        "\n",
        "print(\"\\nFor Pune Restaurants with rating >= 3.8\")\n",
        "print(f\"Mean Price: {mean_prices_yes:.2f}\")\n",
        "print(f\"Standard Deviation: {stdev_prices_yes:.2f}\")\n",
        "print(f\"Number of Instances: {count_prices_yes}\")\n",
        "\n",
        "print(\"\\nFor Pune Restaurants with rating < 3.8\")\n",
        "print(f\"Mean Price: {mean_prices_no:.2f}\")\n",
        "print(f\"Standard Deviation: {stdev_prices_no:.2f}\")\n",
        "print(f\"Number of Instances: {count_prices_no}\")\n",
        "\n",
        "t_value, p_value = stats.ttest_ind(zomato_prices_yes, zomato_prices_no, equal_var=False)\n",
        "print(f\"p-Value: {p_value}\")"
      ],
      "metadata": {
        "id": "NdXZOm4qRGLx"
      },
      "id": "NdXZOm4qRGLx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e2c20d",
      "metadata": {
        "id": "97e2c20d"
      },
      "outputs": [],
      "source": [
        "## Map the cuisines to numerical features according\n",
        "## Instead of mapping 'MISSING': to zero\n",
        "## We now map \"MISSING\" as the (N + 1)th feature\n",
        "\n",
        "def feature_map(feature_df, row_feature):\n",
        "    feature_map = {}\n",
        "\n",
        "    for index, row in feature_df.iterrows():\n",
        "        feature_name = row[row_feature]\n",
        "        rank = row['Rank']\n",
        "\n",
        "        feature_map[feature_name] = rank - 1 ## To account for the requirements of Mixed Naive Bayes\n",
        "\n",
        "    feature_map['MISSING'] = len(feature_map)\n",
        "    ## Heads up: From now on, MISSING will be rank N + 1, but encoded as index N\n",
        "    ## This is to ensure that the behavior of mixed Naive Bayes will function as intended.\n",
        "    return feature_map\n",
        "\n",
        "# Create mappings for cuisine 1 to cuisine 8\n",
        "cuisine1_map = feature_map(eda_resto_cuisine1, 'Cuisine1')\n",
        "cuisine2_map = feature_map(eda_resto_cuisine2, 'Cuisine2')\n",
        "cuisine3_map = feature_map(eda_resto_cuisine3, 'Cuisine3')\n",
        "cuisine4_map = feature_map(eda_resto_cuisine4, 'Cuisine4')\n",
        "cuisine5_map = feature_map(eda_resto_cuisine5, 'Cuisine5')\n",
        "cuisine6_map = feature_map(eda_resto_cuisine6, 'Cuisine6')\n",
        "cuisine7_map = feature_map(eda_resto_cuisine7, 'Cuisine7')\n",
        "cuisine8_map = feature_map(eda_resto_cuisine8, 'Cuisine8')\n",
        "\n",
        "# Create mapping for locality\n",
        "locality_map = feature_map(eda_locality, 'Locality in Pune')\n",
        "\n",
        "# map the columns for cuisine\n",
        "zomato_for_eda['Cuisine1'] = zomato_for_eda['Cuisine1'].map(cuisine1_map)\n",
        "zomato_for_eda['Cuisine2'] = zomato_for_eda['Cuisine2'].map(cuisine2_map)\n",
        "zomato_for_eda['Cuisine3'] = zomato_for_eda['Cuisine3'].map(cuisine3_map)\n",
        "zomato_for_eda['Cuisine4'] = zomato_for_eda['Cuisine4'].map(cuisine4_map)\n",
        "zomato_for_eda['Cuisine5'] = zomato_for_eda['Cuisine5'].map(cuisine5_map)\n",
        "zomato_for_eda['Cuisine6'] = zomato_for_eda['Cuisine6'].map(cuisine6_map)\n",
        "zomato_for_eda['Cuisine7'] = zomato_for_eda['Cuisine7'].map(cuisine7_map)\n",
        "zomato_for_eda['Cuisine8'] = zomato_for_eda['Cuisine8'].map(cuisine8_map)\n",
        "\n",
        "# Map the columns for locality in Pune\n",
        "zomato_for_eda['Locality in Pune'] = zomato_for_eda['Locality in Pune'].map(locality_map)\n",
        "\n",
        "# Create binary classification column\n",
        "zomato_for_eda['isHighRating'] = (zomato_for_eda['Dining_Rating'] >= rating_threshold).astype(int)\n",
        "\n",
        "# Drop the original numerical rating column\n",
        "zomato_for_eda = zomato_for_eda.drop(columns=['Dining_Rating'])\n",
        "\n",
        "# Reorder columns to ensure binary classification is at the rightmost position\n",
        "zomato_for_eda = zomato_for_eda[[col for col in zomato_for_eda.columns if col not in ['isHighRating']] + ['isHighRating']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "357bc516",
      "metadata": {
        "id": "357bc516"
      },
      "outputs": [],
      "source": [
        "input_features = [\"Cuisine1\", \"Cuisine2\", \"Cuisine3\", \"Cuisine4\", \"Cuisine5\", \"Cuisine6\", \"Cuisine7\", \"Cuisine8\", \"Pricing_for_2\", \"Locality in Pune\"]\n",
        "\n",
        "X = zomato_for_eda.filter(items = input_features)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdabbd0",
      "metadata": {
        "id": "1cdabbd0"
      },
      "outputs": [],
      "source": [
        "y = zomato_for_eda[\"isHighRating\"]\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d216f52c",
      "metadata": {
        "id": "d216f52c"
      },
      "source": [
        "### **Post-EDA**\n",
        "\n",
        "After testing various statistical methods for EDA, ...\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8766b69b",
      "metadata": {
        "id": "8766b69b"
      },
      "source": [
        "# **V. Decision Tree Implementation 1 (DT1)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee84e27",
      "metadata": {
        "id": "dee84e27"
      },
      "outputs": [],
      "source": [
        "## 70% train, 30% others (as a working concept, assume that other)\n",
        "X_train, X_other, y_train, y_other = train_test_split(X, y, test_size = 0.3, random_state = 45)\n",
        "\n",
        "dec_tree = DecisionTreeClassifier(random_state = 45)\n",
        "dec_tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"DataFrame with MISSING set to 0:\")\n",
        "print(X_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff287579",
      "metadata": {
        "id": "ff287579"
      },
      "outputs": [],
      "source": [
        "y_pred = dec_tree.predict(X_other)\n",
        "\n",
        "cm = confusion_matrix(y_other, y_pred)\n",
        "cm_display = ConfusionMatrixDisplay(cm, display_labels = dec_tree.classes_)\n",
        "cm_display.plot(cmap = plt.cm.Blues)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f58b9e",
      "metadata": {
        "id": "72f58b9e"
      },
      "outputs": [],
      "source": [
        "tn, fp, fn, tp = cm.ravel()\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "accuracy = accuracy_score(y_other, y_pred)\n",
        "\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"Sensitivity: {sensitivity*100:.2f} %\")\n",
        "print(f\"Specificity: {specificity*100:.2f} %\")\n",
        "print(f\"Accuracy: {accuracy*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def DecisionTreeImplementor1(instance, random_state, accuracy_list, sensitivity_list, specificity_list):\n",
        "\n",
        "  X_train, X_other, y_train, y_other = train_test_split(X, y, test_size = 0.3, random_state = random_state)\n",
        "\n",
        "  # Instantiate last instance\n",
        "  accuracy_list = accuracy_list\n",
        "  sensitivity_list = sensitivity_list\n",
        "  specificity_list = specificity_list\n",
        "\n",
        "  dec_tree = DecisionTreeClassifier(random_state = random_state)\n",
        "  dec_tree.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = dec_tree.predict(X_other)\n",
        "  cm = confusion_matrix(y_other, y_pred)\n",
        "  cm_display = ConfusionMatrixDisplay(cm, display_labels = dec_tree.classes_)\n",
        "  cm_display.plot(cmap = plt.cm.Blues)\n",
        "  plt.show()\n",
        "\n",
        "  tn, fp, fn, tp = cm.ravel()\n",
        "  sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "  specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "  accuracy = accuracy_score(y_other, y_pred)\n",
        "  print(f\"True Positives: {tp}\")\n",
        "  print(f\"Sensitivity: {sensitivity*100:.2f} %\")\n",
        "  print(f\"Specificity: {specificity*100:.2f} %\")\n",
        "  print(f\"Accuracy: {accuracy*100:.2f} %\")\n",
        "\n",
        "  accuracy_list.append(round(100*accuracy, 2))\n",
        "  sensitivity_list.append(float(round(100*sensitivity, 2)))\n",
        "  specificity_list.append(float(round(100*specificity, 2)))\n",
        "\n",
        "  # Updated instantiation\n",
        "  accuracy_list = accuracy_list\n",
        "  sensitivity_list = sensitivity_list\n",
        "  specificity_list = specificity_list\n",
        "\n",
        "  return accuracy_list, sensitivity_list, specificity_list\n",
        "\n",
        "\n",
        "random_state_list = [45, 65, 105, 225, 335]\n",
        "\n",
        "accuracy_list_DT1 = []\n",
        "sensitivity_list_DT1 = []\n",
        "specificity_list_DT1 = []\n",
        "\n",
        "for i in range(len(random_state_list)):\n",
        "  accuracy_list_DT1, sensitivity_list_DT1, specificity_list_DT1 = DecisionTreeImplementor1(i, random_state_list[i], accuracy_list_DT1, sensitivity_list_DT1, specificity_list_DT1)\n",
        "\n",
        "mean_accuracy_DT1 = round(np.mean(accuracy_list_DT1), 2)\n",
        "mean_sensitivity_DT1 = round(np.mean(sensitivity_list_DT1), 2)\n",
        "mean_specificity_DT1 = round(np.mean(specificity_list_DT1), 2)\n",
        "\n",
        "## Sample standard deviation\n",
        "standev_accuracy_DT1 = round(np.std(accuracy_list_DT1, ddof = 1), 2)\n",
        "standev_sensitivity_DT1 = round(np.std(sensitivity_list_DT1, ddof = 1), 2)\n",
        "standev_specificity_DT1 = round(np.std(specificity_list_DT1, ddof = 1), 2)\n",
        "\n",
        "print(f\"For {len(random_state_list)} iterations of Decision Tree Classifier\")\n",
        "print(f\"Accuracy — mean: {mean_accuracy_DT1}%, standard deviation: {standev_accuracy_DT1}%\")\n",
        "print(f\"Sensitivity — mean: {mean_sensitivity_DT1}%, standard deviation: {standev_sensitivity_DT1}%\")\n",
        "print(f\"Specificity — mean: {mean_specificity_DT1}%, standard deviation: {standev_specificity_DT1}%\")"
      ],
      "metadata": {
        "id": "3RRqJVlFl3Rd"
      },
      "id": "3RRqJVlFl3Rd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c5dcba77",
      "metadata": {
        "id": "c5dcba77"
      },
      "source": [
        "The current Decisition Tree (DT1) implementation consists of the basic cleaned data. This means that we've decided to keep the `MISSING` values that occur in the way the data is collected. While restaurants may have 1-2 cuisines, some have over 2 cuisines served. This is the reason why there is `cuisine 1 to 8` in the dataset. As such, `MISSING` values may incur false-negative results in predicting `Dining_Ratings`.\n",
        "\n",
        "We tried making Decision Tree classifiers by performing this across several random states: `45`, `65`, `105`, `225`, `335`. In this case, DT1 has a mean accuracy of `63.50%`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ce3a1c",
      "metadata": {
        "id": "58ce3a1c"
      },
      "source": [
        "## **VI. Decision Tree Implementation 2 (DT2)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917e5aa5",
      "metadata": {
        "id": "917e5aa5"
      },
      "source": [
        "For DT2, the dataset has been further cleaned by removing ` MISSING ` values and making sure that there are no _\"Junk\"_ data that possibly makes Decision Trees lose its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9465968",
      "metadata": {
        "id": "c9465968"
      },
      "outputs": [],
      "source": [
        "# Replace 'MISSING' (0) with -1\n",
        "cuisine_cols = [f'Cuisine{i}' for i in range(1, 9)]\n",
        "X_all = zomato_for_eda.drop(columns=['isHighRating']).copy()\n",
        "X_all[cuisine_cols] = X_all[cuisine_cols].replace(0, -1)\n",
        "\n",
        "# Keep only numeric columns (drop Restaurant_Name etc.)\n",
        "X_clean = X_all.select_dtypes(include=[np.number])\n",
        "y_clean = zomato_for_eda['isHighRating']\n",
        "\n",
        "print(\"Cleaned DataFrame with MISSING features replaced with -1:\")\n",
        "print(X_clean.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577fcb73",
      "metadata": {
        "id": "577fcb73"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train2, X_other2, y_train2, y_other2 = train_test_split(X_clean, y_clean, test_size=0.3, random_state=45)\n",
        "\n",
        "# Train the model\n",
        "dec_tree2 = DecisionTreeClassifier(random_state=45)\n",
        "dec_tree2.fit(X_train2, y_train2)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred2 = dec_tree2.predict(X_other2)\n",
        "cm2 = confusion_matrix(y_other2, y_pred2)\n",
        "cm_display2 = ConfusionMatrixDisplay(cm2, display_labels=dec_tree2.classes_)\n",
        "cm_display2.plot(cmap=plt.cm.Greens)\n",
        "plt.title(\"Confusion Matrix for Decision Tree 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e44f92",
      "metadata": {
        "id": "f2e44f92"
      },
      "outputs": [],
      "source": [
        "# Metrics\n",
        "tn2, fp2, fn2, tp2 = cm2.ravel()\n",
        "sensitivity2 = tp2 / (tp2 + fn2) if (tp2 + fn2) != 0 else 0\n",
        "specificity2 = tn2 / (tn2 + fp2) if (tn2 + fp2) != 0 else 0\n",
        "accuracy2 = accuracy_score(y_other2, y_pred2)\n",
        "\n",
        "print(f\"Decision Tree 2 Metrics (Ignoring MISSING features):\")\n",
        "print(f\"True Positives: {tp2}\")\n",
        "print(f\"Sensitivity: {sensitivity2*100:.2f} %\")\n",
        "print(f\"Specificity: {specificity2*100:.2f} %\")\n",
        "print(f\"Accuracy: {accuracy2*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539297a0",
      "metadata": {
        "id": "539297a0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f15743",
      "metadata": {
        "id": "27f15743"
      },
      "outputs": [],
      "source": [
        "feature_importances = pd.Series(dec_tree2.feature_importances_, index=X_clean.columns)\n",
        "feature_importances = feature_importances.sort_values(ascending=False)\n",
        "print(feature_importances.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1994e5eb",
      "metadata": {
        "id": "1994e5eb"
      },
      "source": [
        "## **Decision Tree Implementation 3 (DT3)**\n",
        "\n",
        "Will removing `Cuisine 3-8` make the model mroe accurate? since, cuisines 3-8 have least importance and the fact that most have missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67492fa9",
      "metadata": {
        "id": "67492fa9"
      },
      "outputs": [],
      "source": [
        "# Keep only Cuisine1, Cuisine2, Pricing_for_2, Locality\n",
        "selected_cuisine_cols = ['Cuisine1', 'Cuisine2']\n",
        "keep_cols = selected_cuisine_cols + ['Pricing_for_2', 'Locality in Pune']\n",
        "\n",
        "X_subset = zomato_for_eda[keep_cols].copy()\n",
        "X_subset[selected_cuisine_cols] = X_subset[selected_cuisine_cols].replace(0, -1)\n",
        "\n",
        "# Keep only numeric columns\n",
        "X_dt3 = X_subset.select_dtypes(include=[np.number])\n",
        "y_dt3 = zomato_for_eda['isHighRating']\n",
        "\n",
        "# Split 70/30\n",
        "X_train3, X_other3, y_train3, y_other3 = train_test_split(X_dt3, y_dt3, test_size=0.3, random_state=45)\n",
        "\n",
        "# Train DT3\n",
        "dec_tree3 = DecisionTreeClassifier(random_state=45)\n",
        "dec_tree3.fit(X_train3, y_train3)\n",
        "\n",
        "# Predict + Confusion Matrix\n",
        "y_pred3 = dec_tree3.predict(X_other3)\n",
        "cm3 = confusion_matrix(y_other3, y_pred3)\n",
        "cm_display3 = ConfusionMatrixDisplay(cm3, display_labels=dec_tree3.classes_)\n",
        "cm_display3.plot(cmap=plt.cm.Oranges)\n",
        "plt.title(\"Confusion Matrix for Decision Tree 3 (Cuisine 1 & 2 only)\")\n",
        "plt.show()\n",
        "\n",
        "# Metrics\n",
        "tn3, fp3, fn3, tp3 = cm3.ravel()\n",
        "sensitivity3 = tp3 / (tp3 + fn3) if (tp3 + fn3) != 0 else 0\n",
        "specificity3 = tn3 / (tn3 + fp3) if (tn3 + fp3) != 0 else 0\n",
        "accuracy3 = accuracy_score(y_other3, y_pred3)\n",
        "\n",
        "print(f\"Decision Tree 3 Metrics (Only Cuisine1 & Cuisine2):\")\n",
        "print(f\"True Positives: {tp3}\")\n",
        "print(f\"Sensitivity: {sensitivity3*100:.2f} %\")\n",
        "print(f\"Specificity: {specificity3*100:.2f} %\")\n",
        "print(f\"Accuracy: {accuracy3*100:.2f} %\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def DecisionTreeImplementor3(instance, random_state, accuracy_list, sensitivity_list, specificity_list):\n",
        "\n",
        "  # Step 1: Keep only Cuisine1, Cuisine2, Pricing_for_2, Locality\n",
        "  selected_cuisine_cols = ['Cuisine1', 'Cuisine2']\n",
        "  keep_cols = selected_cuisine_cols + ['Pricing_for_2', 'Locality in Pune']\n",
        "\n",
        "  X_subset = zomato_for_eda[keep_cols].copy()\n",
        "  X_subset[selected_cuisine_cols] = X_subset[selected_cuisine_cols].replace(0, -1)\n",
        "\n",
        "  # Step 2: Keep only numeric columns\n",
        "  X_dt3 = X_subset.select_dtypes(include=[np.number])\n",
        "  y_dt3 = zomato_for_eda['isHighRating']\n",
        "\n",
        "  # Step 3: Split 70/30\n",
        "  X_train3, X_other3, y_train3, y_other3 = train_test_split(X_dt3, y_dt3, test_size=0.3, random_state=random_state)\n",
        "\n",
        "  # Step 4: Train DT3\n",
        "  dec_tree3 = DecisionTreeClassifier(random_state=45)\n",
        "  dec_tree3.fit(X_train3, y_train3)\n",
        "\n",
        "  y_pred3 = dec_tree3.predict(X_other3)\n",
        "  cm3 = confusion_matrix(y_other3, y_pred3)\n",
        "  cm_display3 = ConfusionMatrixDisplay(cm3, display_labels=dec_tree3.classes_)\n",
        "  cm_display3.plot(cmap=plt.cm.Oranges)\n",
        "  plt.title(\"Confusion Matrix for Decision Tree 3 (Cuisine 1 & 2 only)\")\n",
        "  plt.show()\n",
        "\n",
        "  tn, fp, fn, tp = cm3.ravel()\n",
        "  sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "  specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "  accuracy = accuracy_score(y_other3, y_pred3)\n",
        "  print(f\"True Positives: {tp}\")\n",
        "  print(f\"Sensitivity: {sensitivity*100:.2f} %\")\n",
        "  print(f\"Specificity: {specificity*100:.2f} %\")\n",
        "  print(f\"Accuracy: {accuracy*100:.2f} %\")\n",
        "\n",
        "  accuracy_list.append(round(100*accuracy, 2))\n",
        "  sensitivity_list.append(float(round(100*sensitivity, 2)))\n",
        "  specificity_list.append(float(round(100*specificity, 2)))\n",
        "\n",
        "  # Updated instantiation\n",
        "  accuracy_list = accuracy_list\n",
        "  sensitivity_list = sensitivity_list\n",
        "  specificity_list = specificity_list\n",
        "\n",
        "  return accuracy_list, sensitivity_list, specificity_list\n",
        "\n",
        "\n",
        "random_state_list = [45, 65, 105, 225, 335]\n",
        "\n",
        "accuracy_list_DT3 = []\n",
        "sensitivity_list_DT3 = []\n",
        "specificity_list_DT3 = []\n",
        "\n",
        "for i in range(len(random_state_list)):\n",
        "  accuracy_list, sensitivity_list, specificity_list = DecisionTreeImplementor3(i, random_state_list[i], accuracy_list_DT3, sensitivity_list_DT3, specificity_list_DT3)\n",
        "\n",
        "mean_accuracy_DT3 = round(np.mean(accuracy_list_DT3), 2)\n",
        "mean_sensitivity_DT3 = round(np.mean(sensitivity_list_DT3), 2)\n",
        "mean_specificity_DT3 = round(np.mean(specificity_list_DT3), 2)\n",
        "\n",
        "## Sample standard deviation\n",
        "standev_accuracy_DT3 = round(np.std(accuracy_list_DT3, ddof = 1), 2)\n",
        "standev_sensitivity_DT3 = round(np.std(sensitivity_list_DT3, ddof = 1), 2)\n",
        "standev_specificity_DT3 = round(np.std(specificity_list_DT3, ddof = 1), 2)\n",
        "\n",
        "print(f\"For {len(random_state_list)} iterations of Decision Tree Classifier\")\n",
        "print(f\"Accuracy — mean: {mean_accuracy_DT3}%, standard deviation: {standev_accuracy_DT3}%\")\n",
        "print(f\"Sensitivity — mean: {mean_sensitivity_DT3}%, standard deviation: {standev_sensitivity_DT3}%\")\n",
        "print(f\"Specificity — mean: {mean_specificity_DT3}%, standard deviation: {standev_specificity_DT3}%\")"
      ],
      "metadata": {
        "id": "EUiaI0XeoPzl"
      },
      "id": "EUiaI0XeoPzl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Set variances to be true since we are using the entire sample and train-test-split anway\n",
        "print(\"For Decision 1 and 3\")\n",
        "t_value_accuracy, p_value_accuracy = stats.ttest_ind(accuracy_list_DT1, accuracy_list_DT3, equal_var=True)\n",
        "print(f\"p-Value for accuracy: {round(p_value_accuracy, 6)}\")\n",
        "\n",
        "t_value_sensitivity, p_value_sensitivity = stats.ttest_ind(sensitivity_list_DT1, sensitivity_list_DT3, equal_var=True)\n",
        "print(f\"p-Value for sensitivity: {round(p_value_sensitivity, 6)}\")\n",
        "\n",
        "t_value_specificity, p_value_specificity = stats.ttest_ind(specificity_list_DT1, specificity_list_DT3, equal_var=True)\n",
        "print(f\"p-Value for specificity: {round(p_value_specificity, 6)}\")"
      ],
      "metadata": {
        "id": "7BV0bFOcvxtl"
      },
      "id": "7BV0bFOcvxtl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b2a6f63c",
      "metadata": {
        "id": "b2a6f63c"
      },
      "source": [
        "## **DT Plotting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b3ada3",
      "metadata": {
        "id": "43b3ada3"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(360, 90))  # Resize for visibility\n",
        "plot_tree(dec_tree,\n",
        "          feature_names=X_train.columns,\n",
        "          class_names=[\"Low\", \"High\"],\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=10)\n",
        "plt.title(\"Decision Tree Visualization\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57bf4b80",
      "metadata": {
        "id": "57bf4b80"
      },
      "source": [
        "### **Post Decision Tree Implementation (DT1, DT2, and DT3)**\n",
        "\n",
        "DT2 Implementation findings show us that even ommitting the ` MISSING ` values present the same metrics as it would in DT1. Furthermore, removing `Cuisines 3-8` and focusing on ` Cuisine 1-2 ` barely made significant changes to the accuracy.\n",
        "\n",
        "Effectively, for this study, we can _ignore_ DT2 and focus on DT1 and DT3.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e967c91e",
      "metadata": {
        "id": "e967c91e"
      },
      "source": [
        "# **VII. Mixed Naive Bayes (MNB)**\n",
        "\n",
        "## **Implementing MNB to the cleaned dataset (same with cleaned dataset of DT3)**\n",
        "\n",
        "Mixed Naive Bayes is an implementation of the Naive Bayes classifier where both categorical (Boolean or otherwise) and numerical data are taken together. We assume that the numerical data fall under the Gaussian distribution\n",
        "\n",
        "A Naive Bayes classifier is a classifier that assumes the features are independent of each other.\n",
        "\n",
        "For binary classification...\n",
        "\n",
        "If the feature is categorical, then p(desired label | categorical feature) is the complement of p(undesired label | categorical feature). In shorthand $$p(Y = 1 | x_i) = 1 - p(Y = 0 | x_i) $$\n",
        "\n",
        "Numerical features, they likelihood functions, folllwing the general form $$ L(x_i | Y) = \\frac{1}{σ_i\\sqrt{2\\pi}}e^\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2} $$\n",
        "\n",
        "So, for a Naive Bayes classifier, the proportional probability of a desired label, given features in question are $$ p(Y = 1 | all \\ features \\ x) = p(Y = 1| x_{cat1},x_{cat2},...,x_{catn_1},x_{num1},x_{num2},...,x_{numn_2}) \\propto p(Y = 1) \\cdot \\prod_{i=1}^{n_1} p(x_{cat_i} | Y = 1) \\cdot \\prod_{j=1}^{n_2} L(x_{cat_j} | Y = 1) $$\n",
        "\n",
        "We compute the proportional probability $ p(Y = 0 | all \\ features \\ x) $ as analogous to the formula above.\n",
        "\n",
        "Compare the respective proportions of $ p(Y = 1 | all \\ features \\ x) $ and $ p(Y = 0 | all \\ features \\ x) $ to get the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42955603",
      "metadata": {
        "id": "42955603"
      },
      "source": [
        "# **VIII. General Finding and Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initial Data**\n",
        "\n",
        "**Decision Trees (DT)**\n",
        "The data was tested with the following specifications:\n",
        "\n",
        "- **DT1** - Basic preprocessing transformed the data to numerical, and all `Missing` set to 0 (not `NULL`)\n",
        "- **DT2** - This was removed due to the fact that transforming the `MISSING` values to `-1` does not create any difference from DT1 as initially hypothesized.\n",
        "- **DT3** - Dataset ignored / removed `Cuisines 3-8` due to the majority of the restaurants only having `Cuisines 1-2`.\n",
        "- Each data set were trained with varying `random_states = [45, 65, 105, 225, 335]`\n",
        "\n",
        "<br> **True Values**\n",
        "\n",
        "|              | DT 1             | DT 3             |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 45 |              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 65 |              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 105|              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 225|              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 335|              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "\n",
        "\n",
        "<br> **Confusion Matrix for DT**\n",
        "\n",
        "| Metric       | DT 1             | DT 3             |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 45 |              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 65 |              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 105|              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 225|              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "|--------------|------------------|------------------|\n",
        "|Random State = 335|              |                  |\n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "\n",
        "<br> **For 5 iterations of Decision Tree Classifier**\n",
        "\n",
        "| DT 1 and 2   |   Mean    | Standard Deviation |\n",
        "|--------------|-----------|--------------------|\n",
        "| Accuracy     | 63.5%     | 1.83%              |\n",
        "| Sensitivity  | 47.41%    | 3.27%              |\n",
        "| Specificity  | 71.9%     | 1.77%              |\n",
        "\n",
        "\n",
        "<br> **P-Values for DT Classfiers 1 and 3**\n",
        "\n",
        "| Metric       |  p-Values for DT 1 and 2  |\n",
        "|--------------|---------------------------|\n",
        "| Accuracy     | 0.69297                   |\n",
        "| Sensitivity  | 0.352116                  |\n",
        "| Specificity  | 0.244064                  |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mOschBEEtWiz"
      },
      "id": "mOschBEEtWiz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br> **Mixed Naive Bayes(MNB)**\n",
        "\n",
        "| Metric       | MNB 1            | MNB 2            |\n",
        "|--------------|------------------|------------------|\n",
        "| Means        |                  |                  |  \n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "\n",
        "<br> **Confusion Matrix for MNB**\n",
        "\n",
        "| Metric       | MNB 1            | MNB 2            |\n",
        "|--------------|------------------|------------------|\n",
        "| Means        |                  |                  |  \n",
        "| Accuracy     | a                | B                |\n",
        "| Specificity  | a                | B                |\n",
        "| Sensitivity  | a                | B                |\n",
        "\n",
        "\n",
        "<br> **P-Values for MNB Classfiers 1 and 2**\n",
        "\n",
        "| Metric       |  p-Values for MNB 1 and 2  |\n",
        "|--------------|----------------------------|\n",
        "| Accuracy     | a                          |\n",
        "| Sensitivity  | a                          |\n",
        "| Specificity  | a                          |\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lkVPcpDe0ZDf"
      },
      "id": "lkVPcpDe0ZDf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Mention the statistics of my previous work using Decision Trees and Naive Bayes Classifiers. Make note of why decision trees vary wildly in terms of behavior but the Naive Bayes Classifiers have a pattern of high specificity and lower accuracy.\n",
        "\n",
        "2) Then, if we are up to it, analyze the Naive Bayes classifier, especially when it comes to misclassifications. We can analyze at random 10 false positives and false negatives and link it back to low-sensitivity, high-specificity behavior. I know 10 sounds arbitrary, but I wish we explore why Naive Bayes acts the way it does.\n",
        "\n",
        "3) Only if time permits, do the same for the decision tree (of course, the better performing one)"
      ],
      "metadata": {
        "id": "aoe_A2nxnvmE"
      },
      "id": "aoe_A2nxnvmE"
    },
    {
      "cell_type": "markdown",
      "id": "702983a7",
      "metadata": {
        "id": "702983a7"
      },
      "source": [
        "### **Exploratory Data Analysis (EDA)**\n",
        "EDA presented..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Decision Tree Implementations (DT1, DT2, and DT3)**\n",
        "DT1 and DT2 unfortunately ..."
      ],
      "metadata": {
        "id": "j4V7OkDGpCjf"
      },
      "id": "j4V7OkDGpCjf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mixed Naive Bayes Implementation (MNB1 & MNB2)**\n",
        "MNB presented..."
      ],
      "metadata": {
        "id": "pguJQY-ipEZY"
      },
      "id": "pguJQY-ipEZY"
    },
    {
      "cell_type": "markdown",
      "id": "22fc60b2",
      "metadata": {
        "id": "22fc60b2"
      },
      "source": [
        "# **IX. Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vIIoFh-VpD4H"
      },
      "id": "vIIoFh-VpD4H"
    },
    {
      "cell_type": "markdown",
      "id": "6c59edc5",
      "metadata": {
        "id": "6c59edc5"
      },
      "source": [
        "Based on the findings, it is evident that DT2 and MNB was able to accurately predict... as such, we think it would be more accurate to remove _\"Junk\"_ data such as `MISSING, NULL, -12931931, 13412131` that can affect the model in predicting.\n",
        "\n",
        "In the case of predicting ` dining ratings `, it is possible that there could be a better model to be used since **Decision Trees** and **Mixed Naive Bayes** are only able to get `x%` and `y%`. Judging from the dataset, it may be even viable to use **Ensemble Learning Models (random forest)**, or the need to heavily manipulate the data to accomodate ` MISSING ` values and so on..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9ba363",
      "metadata": {
        "id": "bc9ba363"
      },
      "source": [
        "# **X. References**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GodUZuLrnsjB"
      },
      "id": "GodUZuLrnsjB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1994e5eb",
        "b2a6f63c"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}